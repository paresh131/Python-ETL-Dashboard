# -*- coding: utf-8 -*-
"""RetailSalesVerbessert(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-h_Nx9hHI3gZzZJR2ZuejEZEtxYWJc_5
"""

!pip install dash

!pip install dash-bootstrap-components

import os
import pandas as pd
import sqlite3
import kagglehub

# Database Configuration
DB_FILE = "amazon_sales.db"  # SQLite database file
TABLE_NAME = "sales_data"  # Table where sales data will be stored

def extract_data():
    """
    Extract Amazon Sales Data from Kaggle.
    Downloads the dataset and loads it into a Pandas DataFrame.
    """
    print(" Downloading 'Unlock Profits with E-Commerce Sales Data' from Kaggle...")

    # Dataset Name from Kaggle
    dataset_name = "thedevastator/unlock-profits-with-e-commerce-sales-data"

    # Download dataset from KaggleHub
    path = kagglehub.dataset_download(dataset_name)
    file_path = os.path.join(path, "Amazon Sale Report.csv")  # Ensure correct filename

    # Check if file exists
    if not os.path.exists(file_path):
        print(f" Dataset file NOT found: {file_path}")
        return None

    print(f" Dataset file found: ")

    # Load dataset into Pandas DataFrame
    try:
        data = pd.read_csv(file_path, low_memory=False)
        print(" Data successfully loaded from CSV!")
        return data
    except Exception as e:
        print(f" Error while reading CSV file: {e}")
        return None

def transform_data(data):
    """
    Clean and transform the extracted sales data.
    - Handles missing values
    - Converts column names to lowercase
    - Converts 'Qty' & 'Amount' to numeric types
    - Creates a new column 'Total Revenue' = Qty * Amount
    """
    if data is None:
        print(" No data available for transformation.")
        return None

    print(" Cleaning and transforming sales data...")

    # Convert all column names to lowercase for consistency
    data.columns = map(str.lower, data.columns)

    # Handle missing values by replacing with 0
    data.fillna(0, inplace=True)

    # Ensure required columns exist before proceeding
    required_cols = ['qty', 'amount']
    if not all(col in data.columns for col in required_cols):
        print(f" Required columns missing: {required_cols}")
        return None

    # Convert 'qty' and 'amount' to numeric, replacing errors with 0
    data['qty'] = pd.to_numeric(data['qty'], errors='coerce').fillna(0)
    data['amount'] = pd.to_numeric(data['amount'], errors='coerce').fillna(0)

    # Create a new column 'Total Revenue'
    data['total revenue'] = data['qty'] * data['amount']

    print(" Data transformation complete!")
    return data

def load_data(data):
    """
    Load the cleaned sales data into an SQLite database.
    """
    if data is None:
        print(" No transformed data available to load into the database.")
        return

    print(" Storing transformed sales data into SQLite database...")

    try:
        with sqlite3.connect(DB_FILE) as conn:
            data.to_sql(TABLE_NAME, conn, if_exists='replace', index=False)
            print(f" Data successfully stored in {DB_FILE}, table: {TABLE_NAME}")
    except Exception as e:
        print(f" Error while inserting data into database: {e}")

def run_etl_pipeline():
    """
    Execute the full ETL (Extract, Transform, Load) pipeline.
    """
    print("Running full ETL pipeline...")

    # Extract
    raw_data = extract_data()

    # Transform
    cleaned_data = transform_data(raw_data)

    # Load
    load_data(cleaned_data)

    print(" ETL pipeline successfully completed!")

# Execute ETL Process
run_etl_pipeline()

